<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>DLAS3D</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="deepl3d final" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Deep Learning Final project</span>
		<table align=center width=250px>
			<table align=center width=250px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://github.com/CptWang">Rui Wang</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href="http://www.mechhu.com">Bingwen Hu</a></span>
						</center>

					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Video]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/hu-bingwen/DLAS3D/'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					This is a final project for <a href="https://courses.cs.washington.edu/courses/cse490g1/22au/">cse490g1</a>. The code can be found in this <a href="https://github.com/hu-bingwen/deepl3d">repository</a>.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Semantic segmentation is useful to segment out different parts in tissues from microscopic biomedical images. However, it is a tough task to do traditional computer vision-based segmentation in 3D since both the continuity and context information need to be considered for all three directions, many parameters need to be taken into account and much manual examination and tweaking make it tedious and time-consuming.
Deep learning-based semantic segmentation remains as a promising technology to deal with such problem since the models generate well on different datasets with similar properties. Once well trained, no more manual efforts need to be done. There are already a lot of successful deep learning models for specific segmentation tasks of 3D biomedical images.
nnU-Net is a self-configuring model for various biomedical segmentation tasks. It can configure different network structures, pre/post-processing pipelines, etc based on the dataset properties like imaging modality, image sizes, voxel spacings, class ratios, etc. In this project, we adapt nnU-Net to train a model which is capable of segmenting out epithelium and lumen of synthetic immunolabeled prostate glands in 3D images.

			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Video</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:2
				
				8px"><a href='https://docs.google.com/presentation/d/1jaxdqSjMDxi2_P4TskohABNyrz39Gl2gDmGbc5vZChM/edit?usp=sharing'>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>

	<hr>

	<table align=center width=850px>
		<center><h1>Problem statement</h1></center>
		<!-- <table align=center width=850px> -->
		<tr>
			<td width=260px>
				<center>
					<img class="round" style="width:500px" src="./resources/problem_statement.jpg"/>
				</center>
			</td>
		</tr>

		<tr>
			<td>
				Our lab’s former member, Dr. Weisi Xie, created an image translation-assisted segmentation in 3D (ITAS3D) method to segment out different parts (epithelium, lumen, stroma, etc) within 3D images of cancerous prostate biopsies, thus to help pathologists grade the severity of the cancer. This whole pipeline is great and helpful, but a bit sophisticated. The final step of the pipeline for gland segmentation is based on traditional computer vision methods which requires a lot of manual parameter tweaking, we need to do trial-and-errors multiple times when processing one biopsy, to see the intermediate output and tweak those parameters accordingly for better results, which is tedious and time-consuming. What’s more tedious and time-consuming is that you need to do the manual parameter tweaking for each and every single biopsy as those parameters don’t generate and adapt well for different cases. Thus, we are always thinking about utilizing deep learning to train another model to automatically do the segmentation.
Our 3D prostate biopsy images are taken by lightsheet microscopes, but major 3D biomedical segmentation models are trained on CT or MRI images. Transfer learning with those models may require high levels of expertise and experience, with small errors leading to large drops in performance, some successful configurations from one dataset rarely translate to another. To address these issues, we found the latest self-configuring biomedical image segmentation model called nnU-Net, which can adapt itself no matter on preprocessing, network architecture, training and post-processing for any unseen task, thus perfectly suits our needs. So, we will use this method to train a model that can segment out epithelium and lumen within 3D cancerous prostate biopsies, to replace the current traditional CV-based segmentation process.

			</td>
		</tr>
	</table>
	<br>

	<hr>
	<table align=center width=850px>
		<center><h1>Datasets</h1></center>
		<tr>
			<td>
				The original data were taken by the lightsheet microscopes in our lab which contains nuclei and cytoplasm channels. We used GAN to translate those channels into synthetic immunolabeled images which are easier for segmentation tasks. For memory limit reasons, we use 4x downsampled 3D images (still got ~7000*512*350 in pixel size) for training and inference. There are more than 100 biopsies with ground truth segmentation masks generated previously by traditional thresholding-based CV segmentation methods that we can use for training in a supervised manner. The input are synthetic immunolabeled images and the target are those corresponding ground truth segmentation masks.			</td>
		</tr>
	</table>
	<br>

	<hr>
	<table align=center width=850px>
		<center><h1>Related work</h1></center>
		<tr>
			<td>
				Our concepts are inspired by the paper <a href='https://washington-seattle.digication.com/readfile.digi?localfile=M6b5df1b655fd18f245ea99b6b252d403&filename=334.full.pdf'>"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"</a> published on the <em>Nature Method</em> and the paper <a href='https://www.nature.com/articles/s41592-020-01008-z'>"Prostate CancerRiskStratificationviaNondestructive3D Pathology with Deep Learning–Assisted Gland Analysis"</a> published on the <em>Cancer research </em>
			</td>
		</tr>
	</table>
	<br>
	<!-- <hr> -->

	<table align=center width=450px>
		<!-- <center><h1>Supplementary Material</h1></center> -->
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper2.png"/></a></td>
			<td><span style="font-size:14pt">Weisi Xie, et al.<br>
				<b>Prostate CancerRiskStratificationviaNondestructive3D Pathology with Deep Learning–Assisted Gland Analysis</b><br>
				Cancer research, 2021.<br>
				( <a href="https://washington-seattle.digication.com/readfile.digi?localfile=M6b5df1b655fd18f245ea99b6b252d403&filename=334.full.pdf">PDF</a>)<br>
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<!-- <hr> -->
	<br>

	<table align=center width=450px>
		<!-- <center><h1>Supplementary Material</h1></center> -->
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al.<br>
				<b>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</b><br>
				Nature Methods, 2021.<br>
				( <a href="https://www.nature.com/articles/s41592-020-01008-z">PDF</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<!-- <hr> -->
	<br>


<!-- 
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table> -->

	<hr>
	<!-- <br> -->

	<table align=center width=850px>
		<center><h1>Methodology</h1></center>
		<tr>
			<td>
				We employed the nnU-Net repository and follow the instruction to install the whole framework for our use. Then we prepared the data and restructure them to the correct format and arrangement for training and inference. Our 3D image data are still very big after 4x downsample (~7000*512*350 in pixel size) thus cannot be directly fed into the network due to memory limit and time concern. Hence, we chunked our data into ~512*512*350 blocks from 4 entire biopsies to form the training and inference dataset. This ensured that no out of memory issue would occur during training and inference, as well as guaranteeing a reasonable training time so we could get some results before the project due, but it will impact on the model’s prediction accuracy since some context information would be lost after chunking the image data. The model wouldn’t be able to “see” the entire biopsy as a whole while training and might interpret some lumen field as stroma. Ultimately, we will utilize more computing power from our lab’s workstation that can take in much larger input and train a lot longer time to generate a more well-performed model in the future. The evaluation of the model is integrated in the training process using Dice coefficient as the performance metric. We will also examine the segmentation results generated by the model after inference to visually evaluate how much difference against the ground truth masks.
			</td>
		</tr>
	</table>
	<br>


	<!-- <center><h1>Code</h1></center> -->

	
	<!-- <table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:450px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table> -->

	<hr>
	<table align=center width=850px>
		<center><h1>Experiments</h1></center>
		<tr>
			<td>
				Our concepts are inspired by the paper <a href='https://www.nature.com/articles/s41592-020-01008-z'>"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"</a> published on the <em>Nature Method</em>.
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:450px" src="./resources/training_progress.jpg"/></td>
				</center>
			</td>
		</tr>
	</table>

	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:450px" src="./resources/evaluation.png"/></td>
				</center>
			</td>
		</tr>
	</table>




	<hr>
	<table align=center width=850px>
		<center><h1>Results</h1></center>
		<tr>
			<td width=260px>
				<center>
					<img src="./resources/segmusk_ground.gif" alt="Segementation musk - Ground truth" style="width:48px;height:48px;">
				</center>
			</td>

			<td width=260px>
				<center>
					<img src="./resources/segmusk_model.gif" alt="Segementation musk - output by the model" style="width:48px;height:48px;">
				</center>
			</td>
		</tr>

	<table>

		<tr>
			<td>
				Our concepts are inspired by the paper <a href='https://www.nature.com/articles/s41592-020-01008-z'>"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"</a> published on the <em>Nature Method</em>.
			</td>
		</tr>

	</table>
	<br>

	<hr>
	<table align=center width=850px>
		<center><h1>Conclusion</h1></center>
		<tr>
			<td>
				Our concepts are inspired by the paper <a href='https://www.nature.com/articles/s41592-020-01008-z'>"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"</a> published on the <em>Nature Method</em>.
			</td>
		</tr>
	</table>
	<br>



	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Short description if wanted
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/hu-bingwen/DLAS3D/'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>
	<hr>
	

	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> The code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

